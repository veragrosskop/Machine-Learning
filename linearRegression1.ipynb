{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuAFgZAhuABgZghsARub/AN0MPJbkyvZZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10abf9cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019361400435\n"
     ]
    }
   ],
   "source": [
    "#cost function\n",
    "def cost(theta0, theta1, x, y): #cost function\n",
    "    x_predict = theta0 + theta1 * x #defined by the hypothesis function above\n",
    "    J = sum((x_predict - y)**2)/(2*size(x_predict))\n",
    "    #print(size(x),size(x_predict)) #test \n",
    "    return J\n",
    "\n",
    "print(cost(2, 0.5, x, y) - (0.3**2/2)) #determines how close the value is\n",
    "#prints 0.0019361400435, which is decently close!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(x_predict, theta0, theta1, x, y):\n",
    "    dtheta0 = mean(x_predict-y)\n",
    "    dtheta1 = dot((x_predict-y),x)/size(x)\n",
    "    return dtheta0, dtheta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2482610496 >= 5.15478388374\n"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0 #initialize theta values\n",
    "iterations = 2    \n",
    "\n",
    "#test\n",
    "#iterations = 2, gave : 11.2482610496 >= 5.15478388374\n",
    "#therefore the functions seemed to work.\n",
    "\n",
    "costbefore = cost(theta0, theta1, x, y) #11.2482610496 for example\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    (dtheta0, dtheta1) = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "    costafter = cost(theta0, theta1, x, y)\n",
    "    \n",
    "    theta0 = theta0 - alpha*dtheta0\n",
    "    theta1 = theta1 - alpha*dtheta1\n",
    "    \n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
